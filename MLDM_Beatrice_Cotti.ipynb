{"cells":[{"cell_type":"markdown","metadata":{"id":"fQAuFIojY4Ac"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"rKE3YgkmY9H1"},"source":["The objective of this project is to compare various classication algorithms (both basic and ensembles),\n","when applied to the *Online Shoppers Purchasing Intention Dataset*,\n","which can be found at the UCI Machine Learning Repository at the following link:\n","https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset.\n","\n","The dataset contains information about the visitors of a shopping website, including metrics taken from Google Analytics.\n","The objective is to predict wheter a certain visitor will generate revenue or not: this could allow to offer certain\n","content only to those who intend to purchase and not to the other users.\n","\n","The dataset was formed so that each session would belong to a different user in a 1-year period to avoid\n","any tendency to a specific campaign, special day, user profile, or period.\n","\n","The original authors specified that the dataset is imbalanced, so special care will be required to avoid excessive bias towards the majority class.\n"]},{"cell_type":"markdown","metadata":{"id":"cwIqoaD8QtNl"},"source":["# Initial Setup"]},{"cell_type":"markdown","metadata":{"id":"p3oVkntVZrWF"},"source":["## Imports"]},{"cell_type":"markdown","metadata":{"id":"E39qASY1QtNm"},"source":["We need to import a few common modules, initialize random seeds,\n","ensure MatplotLib plots figures are inline and we also need to prepare a function to save the figures.\n","We also check that Python 3.5 or later is installed, as well as Scikit-Learn ≥0.20."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxSBkpE8QtNn"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import pandas as pd\n","import os\n","import tensorflow as tf\n","import numpy as np\n","import sklearn\n","import sys\n","\n","# Python ≥3.5 is required\n","assert sys.version_info >= (3, 5)\n","\n","# Scikit-Learn ≥0.20 is required\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# Initialize random seeds\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","mpl.rc(\"axes\", labelsize=14)\n","mpl.rc(\"xtick\", labelsize=12)\n","mpl.rc(\"ytick\", labelsize=12)\n","\n","# Where to save the figures\n","ROOT_DIR = \".\"\n","IMAGES_PATH = os.path.join(ROOT_DIR, \"images\")\n","os.makedirs(IMAGES_PATH, exist_ok=True)\n","\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    \"\"\" Saves a figure \"\"\"\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)\n"]},{"cell_type":"markdown","metadata":{"id":"FLBjsIU-QtNo"},"source":["## Download Dataset"]},{"cell_type":"markdown","metadata":{"id":"63p46wlslJh1"},"source":["Since the dataset is provided as a `.csv` file, we can use the `read_csv` function of `pandas` to import it directly from the URL."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_OR5V8LQtNq"},"outputs":[],"source":["DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv\"\n","dataset = pd.read_csv(DATASET_URL)\n"]},{"cell_type":"markdown","metadata":{"id":"H-Fy1Gg-QtNv"},"source":["### Structure of the dataset"]},{"cell_type":"markdown","metadata":{"id":"xiZRgj2LlJh2"},"source":["We can use the `info` method to print a concise summary of the imported dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svWVlYYDMXYO"},"outputs":[],"source":["dataset.info()\n"]},{"cell_type":"markdown","metadata":{"id":"rGPb0qepMd-o"},"source":["The dataset contains a total of `12330` entries, with `18` features.\n","Some features are numeric, others are categorical.\n","\n","`Administrative`, `Administrative Duration`, `Informational`, `Informational Duration`, `Product Related` and `Product Related Duration` represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories.\n","\n","The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another.\n","\n","The `Bounce Rate`, `Exit Rate` and `Page Value` features represent the metrics measured by Google Analytics for each page in the e-commerce site.\n","\n","The value of `Bounce Rate` feature for a web page refers to the percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session.\n","\n","The value of `Exit Rate` feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session.\n","\n","The `Page Value` feature represents the average value for a web page that a user visited before completing an e-commerce transaction.\n","\n","The `Special Day` feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction.\n","The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentine’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.\n","\n","`Revenue` represents the class of the instance: a `True` value means the user generated revenue, and a `False` value means the user did not generate revenue.\n","\n","The dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year."]},{"cell_type":"markdown","metadata":{"id":"uzonLxQalJh3"},"source":["We can get a glimpse of the data by using the `head` method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNbOZZHAQtNv"},"outputs":[],"source":["dataset.head()\n"]},{"cell_type":"markdown","metadata":{"id":"PF-rD-0gQrhK"},"source":["It's also important to check if the number of instances in each class is balanced:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfjSXnfUQc1Q"},"outputs":[],"source":["dataset[\"Revenue\"].value_counts(normalize=True)\n"]},{"cell_type":"markdown","metadata":{"id":"9ArMNxC3RGJu"},"source":["The `84.5%` of instances are negative, while the the `15.5%` are positive.\n","This means that the dataset is imbalanced.\n","\n","There are a few different ways to handle imbalanced datasets,\n","such as *undersampling* the majority class, or *oversampling* the minority one.\n","We could also use more advanced algorithms, such as *SMOTE*, to generate synthetic samples from the minority class.\n","\n","In this project, we will not make use of such techniques directly, but we will\n","tune the algorithms to account for the imbalance as much as possible,\n","and we will also use algorithms created specifically for imbalanced datasets.\n","Most of these algorithms do use one (or more) of the methods mentioned above.\n","\n","Rather than the basic *accuracy* metric, we will use *balanced accuracy*, which is suited for imbalanced data.\n","*Balanced accuracy* is defined as the arithmetic mean of *accuracy* and *recall*:\n","\n","$$\n","\\text{Balanced Accuracy} = \\frac{sensitivity + specificity}{2}\n","$$\n","\n","We could also use the *F1 score* metric, which is the harmonic mean of *precision* and *recall*:\n","\n","$$\n","\\text{F1 Score} = 2 * \\frac{precision * sensitivity}{precision + sensitivity}\n","$$\n","\n","Where *sensitivity* is the proportion of actual positives that are correctly identified as such,\n","*specificity* is the proportion of actual negatives that are correctly identified,\n","and *precision* quantifies the number of correct positive predictions made out of positive predictions made by the model.\n","\n","$$\n","\\text{Sensitivity} = \\frac{TP}{TP + FN}\n","\\hspace{6 mm}\n","\\text{Specificity} = \\frac{TN}{TN + FP}\n","\\hspace{6 mm}\n","\\text{Precision} = \\frac{TP}{TP + FP}\n","$$\n","\n","*F1 score* however doesn’t care about how many true negatives are being classified.\n","For the purposes of this project positives are as import as negatives, so *balanced accuracy* is a better metric."]},{"cell_type":"markdown","metadata":{"id":"k33XbElQ7mWb"},"source":["# Data Cleaning and Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"rV09yQBPaI4q"},"source":["## Column names"]},{"cell_type":"markdown","metadata":{"id":"OJh8f1q2aMDu"},"source":["The column naming convention appears to be inconsistent.\n","We can begin the data cleaning process by converting all the column names to *snake_case*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFog_gpcV63R"},"outputs":[],"source":["def to_snake_case(str):\n","    res = [str[0].lower()]\n","    for i, c in enumerate(str[1:]):\n","        if c in ('ABCDEFGHIJKLMNOPQRSTUVWXYZ'):\n","            if str[i] != \"_\":\n","                res.append('_')\n","            res.append(c.lower())\n","        else:\n","            res.append(c)\n","\n","    return ''.join(res)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yg4yECvRlJh6"},"outputs":[],"source":["dataset.columns = dataset.columns.map(lambda c: to_snake_case(c))\n"]},{"cell_type":"markdown","metadata":{"id":"feXvsTHcbIHn"},"source":["## Column Types"]},{"cell_type":"markdown","metadata":{"id":"t1fzy9R0bcCY"},"source":["There are a few columns that represent categorical data, and two boolean columns that could cause problems. We can convert such columns to more convenient data types."]},{"cell_type":"markdown","metadata":{"id":"UspP5qDpM4Hu"},"source":["### Convert categorical data"]},{"cell_type":"markdown","metadata":{"id":"PnSupd6gVg6V"},"source":["The dataset has two categorical features with string values: `Month` and `VisitorType`.\n","We can observe the possible values of these features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDSRgxl3VBAL"},"outputs":[],"source":["print(dataset[\"month\"].unique())\n","print(dataset[\"visitor_type\"].unique())\n"]},{"cell_type":"markdown","metadata":{"id":"kcCMP_zYTK23"},"source":["These features can be converted into integers using `sklearn.preprocessing.LabelEncoder()`, which replaces the category string values with increasing integers values.\n","\n","The problem with this method is that the learning algorithms could interpret the integer values as having an order/hierarchy between them.\n","This is fine for the `Month` column, where the various months will be encoded with integers between 0 and 11: these integers do have a meaningful order, so label encoding is correct.\n","\n","However for `VisitorType` the integer values that the feature could have to represent the string values have no order/hierarchy.\n","For this reason, a more appropriate conversion for this feature would be to use one-hot encoding.\n","In this strategy, each possible category value is converted into a new column\n","and assigned a 1 or 0 value depending on the value in the original column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XZ6i8X_Fn-Q"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","dataset[\"month\"] = le.fit_transform(dataset[\"month\"])\n","\n","dum_df = pd.get_dummies(dataset[\"visitor_type\"], prefix=\"visitor_type\")\n","dum_df.columns = dum_df.columns.map(lambda c: to_snake_case(c))\n","\n","dataset = dataset.join(dum_df).drop(\"visitor_type\", axis=1)\n"]},{"cell_type":"markdown","metadata":{"id":"lf9_S-b17a6C"},"source":["### Convert boolean values to integer"]},{"cell_type":"markdown","metadata":{"id":"Y1oNe62X73eP"},"source":["The `weekend` and `revenue` columns have boolean values, which should automatically be converted to `0` and `1` by Python, but since some of the algorithms may have some parts implemented in C/C++, we might run into some problems.\n","\n","To avoid this, we can convert such columns to integer values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Od_LBGf78Sv-"},"outputs":[],"source":["dataset[\"weekend\"] = dataset[\"weekend\"].astype(int)\n","dataset[\"revenue\"] = dataset[\"revenue\"].astype(int)\n"]},{"cell_type":"markdown","metadata":{"id":"aaToOTjfHN5T"},"source":["## Missing values"]},{"cell_type":"markdown","metadata":{"id":"p1H9XnyXIsDX"},"source":["We can check if the dataset contains NA values, and if it does we can delete such rows as they might ruin the learning process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-pTdET8HP6r"},"outputs":[],"source":["dataset.isna().any()\n"]},{"cell_type":"markdown","metadata":{"id":"-eIUae-PIxUV"},"source":["The dataset does not contain any NA value, so no rows need to be eliminated."]},{"cell_type":"markdown","metadata":{"id":"0JoKHo7mQjuu"},"source":["## Correlated Columns"]},{"cell_type":"markdown","metadata":{"id":"S318L6-Fd_r9"},"source":["To increase the speed of the learning process and reduce bias a possibility is to remove highly correlated columns from the dataset. \n","\n","However, while developing this project, we discovered *Recursive Feature Elimination*, which provides a more effective way of removing unnecessary features.\n","\n","We are leaving this section here anyway for compleness."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrUdNksjQl2B"},"outputs":[],"source":["def get_correlated_cols(ds: pd.DataFrame, corr_threshold: float):\n","    # Compute correlation matrix using pearson method (linear correlation)\n","    corr = ds.corr(method=\"pearson\")\n","    # Find collinear columns\n","    corr_cols = corr[corr > corr_threshold].dropna(\n","        thresh=2).dropna(axis=\"columns\")\n","    return corr_cols\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5UAxthaRExI"},"outputs":[],"source":["get_correlated_cols(dataset, 0.90)\n"]},{"cell_type":"markdown","metadata":{"id":"95GOZ3qXenPH"},"source":["`bounce_rates` and `exit_rates` are highly correlated. We can remove one of the two columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fFw_roYZTIel"},"outputs":[],"source":["# dataset.drop(\"exit_rates\", axis=1, inplace=True)\n"]},{"cell_type":"markdown","metadata":{"id":"vmNnHhHVJG5A"},"source":["## Duplicate Rows"]},{"cell_type":"markdown","metadata":{"id":"7Di1-yi7KATE"},"source":["It's good practice to identify and remove duplicate rows in the dataset, because they could result in misleading performance when evaluating ML algorithms: duplicate rows could appear in both train and test datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfuHkzlJJY_p"},"outputs":[],"source":["dups = dataset.duplicated()\n","print(dups.any())\n"]},{"cell_type":"markdown","metadata":{"id":"XeUb-QWyJqUn"},"source":["There are duplicates in the dataset. They can be removed using pandas `drop_duplicates`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lhq4ggcvJm8c"},"outputs":[],"source":["dataset.drop_duplicates(inplace=True)\n"]},{"cell_type":"markdown","metadata":{"id":"FeHkqxul9dMx"},"source":["## Feature scaling"]},{"cell_type":"markdown","metadata":{"id":"xQOliItYfoys"},"source":["In order to weight the features equally, feature scaling is important.\n","Here we use the `MinMaxScaler`, which scales all values to the [0,1] interval."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fd2rqh2C9joY"},"outputs":[],"source":["from pandas.core.common import random_state\n","from sklearn.preprocessing import MinMaxScaler\n","\n","mms = MinMaxScaler()\n","dataset = pd.DataFrame(mms.fit_transform(dataset), columns=dataset.columns)\n"]},{"cell_type":"markdown","metadata":{"id":"Nxn0hNB3QtNy"},"source":["## Create test and training sets"]},{"cell_type":"markdown","metadata":{"id":"3YrloRVYJjTI"},"source":["We will use the 70% of the total instances for training, and the remaining 30% for testing.\n","\n","The `stratify` option of `train_test_split` ensures that relative class frequencies are approximately preserved in the training and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fs433X00QtN1"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X = dataset.drop(\"revenue\", axis=1)\n","y = dataset[\"revenue\"].copy()\n","\n","X_train, X_test, y_train,  y_test = train_test_split(\n","    X, y, test_size=0.3, stratify=y, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"jSEn7BfKXfhl"},"source":["## Recursive Feature Elimination"]},{"cell_type":"markdown","metadata":{"id":"7nZiyVStXrPd"},"source":["*Recursive Feature Elimination* (RFE) is a feature selection algorithms.\n","Feature selection refers to techniques that select a subset of the most relevant features (columns) for a dataset. Fewer features can allow machine learning algorithms to run more efficiently (less space or time complexity) and be more effective. Some machine learning algorithms can be misled by irrelevant input features, resulting in worse predictive performance.\n","\n","RFE in particular searches for a subset of features by starting with all features in the training dataset and successfully removing features until the desired number remains.\n","\n","This is achieved by fitting a given machine learning algorithm, ranking features by importance, discarding the least important features, and re-fitting the model. This process is repeated until a specified number of features remains.\n","\n","Choosing the optimal number of features to keep is not trivial: \n","`scikit-learn` provides the `RFECV` class, which performs cross-validation evaluation of different numbers of features and automatically selects the features that resulted in the best mean score.\n","\n","Since we know that the dataset is imbalanced, we use a random forest classifier to compute the weights associated with the features (the features with the lowest weights are those that will be removed), along with the `balanced_accuracy` to measure performance during the cross-validation. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMFYG07SRm2h"},"outputs":[],"source":["from sklearn.feature_selection import RFECV\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rfecv = RFECV(\n","    estimator=RandomForestClassifier(random_state=42),\n","    min_features_to_select=1,  # Eliminate at least one feature\n","    scoring=\"balanced_accuracy\"\n",")\n","\n","rfecv.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rv7oMd8sYa45"},"outputs":[],"source":["print(\"Eliminated %d features, from %d features to %d features\" % (\n","    len(X.columns) - rfecv.n_features_,\n","    len(X.columns),\n","    rfecv.n_features_\n",")\n",")\n","\n","columns_to_keep = X.columns[rfecv.support_]\n","\n","X = X[columns_to_keep]\n","X_train = X_train[columns_to_keep]\n","X_test = X_test[columns_to_keep]\n"]},{"cell_type":"markdown","metadata":{"id":"pumyaOvphvc7"},"source":["## Utility functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PAuD9p1Ph193"},"outputs":[],"source":["from sklearn.metrics import balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","\n","\n","def print_confusion_matrix(y_true, y_pred):\n","    ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(\n","        y_true, y_pred), display_labels=[\"No Revenue\", \"Revenue\"]).plot()\n","\n","\n","def evaluate(clf):\n","    \"\"\"\n","    Evaluates a classifier on the test set.\n","    \"\"\"\n","    y_pred = clf.predict(X_test)\n","    print(\"Accuracy (on test set): \", balanced_accuracy_score(y_test, y_pred))\n","\n","\n","def evaluate_grid(grid_clf):\n","    \"\"\"\n","    Evaluates a grid search on the test set.\n","    \"\"\"\n","    y_pred = grid_clf.predict(X_test)\n","    print(\"Best parameters: \", grid_clf.best_params_)\n","    print(\"Accuracy of best (means of cross-validated scores on train set): \",\n","          grid_clf.best_score_)\n","    print(\"Accuracy of best (on test set): \",\n","          balanced_accuracy_score(y_test, y_pred))\n","    print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"oSgeqcnKlaCb"},"source":["# Basic Classifiers"]},{"cell_type":"markdown","metadata":{"id":"2SlW8I8Qkyhc"},"source":["## Decision Tree"]},{"cell_type":"markdown","metadata":{"id":"-KN8rmq_7dCu"},"source":["We expect decision trees to behave well for this problem, because they are particularly suited for imbalanced classifications.\n","\n","We will begin with a baseline decision tree, that we will try to improve upon by tuning the hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UsWoThHuoL3r"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","# class_weight=balanced adjusts weights inversely proportional to class frequencies\n","tree_clf = DecisionTreeClassifier(\n","    max_depth=2, random_state=42, class_weight=\"balanced\")\n","tree_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snKquz73pknD"},"outputs":[],"source":["evaluate(tree_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"rGX8ddr2aY4g"},"source":["We can also visualize the decision tree:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQe0rzjtkfaB"},"outputs":[],"source":["from graphviz import Source\n","from sklearn.tree import export_graphviz\n","\n","export_graphviz(\n","    tree_clf,\n","    out_file=os.path.join(IMAGES_PATH, \"tree.dot\"),\n","    feature_names=X.columns,\n","    class_names=[\"No Revenue\", \"Revenue\"],\n","    filled=True,\n","    rounded=True,\n",")\n","\n","Source.from_file(os.path.join(IMAGES_PATH, \"tree.dot\"))\n"]},{"cell_type":"markdown","metadata":{"id":"stQqHgH77xJD"},"source":["To check for overfitting, we can plot the accuracy on the training and test sets.\n","Here we also evaluate how the decision trees behaves when using different criteria."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4MEkvyEfyL1"},"outputs":[],"source":["def evaluate_decision_tree(values, criterion=\"gini\"):\n","    train_scores, test_scores = list(), list()\n","\n","    for i in values:\n","        # configure the model\n","        model = DecisionTreeClassifier(\n","            max_depth=i, criterion=criterion, random_state=42)\n","        # fit model on the training dataset\n","        model.fit(X_train, y_train)\n","        # evaluate on the train dataset\n","        train_yhat = model.predict(X_train)\n","        train_acc = balanced_accuracy_score(y_train, train_yhat)\n","        train_scores.append(train_acc)\n","\n","        # evaluate on the test dataset\n","        test_yhat = model.predict(X_test)\n","        test_acc = balanced_accuracy_score(y_test, test_yhat)\n","        test_scores.append(test_acc)\n","        # summarize progress\n","        print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n","    # plot of train and test scores vs tree depth\n","    plt.plot(values, train_scores, '-o', label='Train')\n","    plt.plot(values, test_scores, '-o', label='Test')\n","    plt.legend()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"147A9V1cidIW"},"outputs":[],"source":["evaluate_decision_tree([i for i in range(5, 30)])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiVVr8B2iziG"},"outputs":[],"source":["evaluate_decision_tree([i for i in range(5, 30)], criterion=\"entropy\")\n"]},{"cell_type":"markdown","metadata":{"id":"zfXVdFaL8n0u"},"source":["For a more comprehensive hyperparameter tuning, `scikit-learn` provides the `GridSearchCV` class, which creates a grid search to find the best possibile hyperparameters for a model (by exhaustively trying all possible combinations of the given parameters), and evaluates each possible model through cross-validation, in order to keep overfitting in check. The model with the highest cross-validated score will be kept as the best one.\n","\n","We also evaluate the (balanced) accuracy of the best model using the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vWVnx8MPIPq"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","params = {\n","    \"criterion\": [\"gini\", \"entropy\"],\n","    \"max_depth\": list(range(1, 10)),\n","    \"max_features\": [None, \"log2\", \"sqrt\"],\n","}\n","\n","\n","# class_weight=balanced adjusts weights inversely proportional to class frequencies\n","tree_clf = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\")\n","grid_tree_clf = GridSearchCV(tree_clf, params, scoring=\"balanced_accuracy\")\n","grid_tree_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_I-DWGhppVA"},"outputs":[],"source":["evaluate_grid(grid_tree_clf)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvrMX71RlJiA"},"outputs":[],"source":["export_graphviz(\n","    grid_tree_clf.best_estimator_,\n","    out_file=os.path.join(IMAGES_PATH, \"best_tree.dot\"),\n","    feature_names=X.columns,\n","    class_names=[\"No Revenue\", \"Revenue\"],\n","    filled=True,\n","    rounded=True,\n",")\n","\n","Source.from_file(os.path.join(IMAGES_PATH, \"best_tree.dot\"))\n"]},{"cell_type":"markdown","metadata":{"id":"OPGq2ec-8YTT"},"source":["## K-Nearest Neighbor"]},{"cell_type":"markdown","metadata":{"id":"hAao-3eklJiA"},"source":["The *K-Nearest Neighbor* (KNN) algorithm (at least in its basic form) struggles with imbalanced data, but at the same it should also perform particularly well for datasets with a lower number of features.\n","Thanks to RFE, we managed to reduce the number of features to 9, so it is interesting to verify the results of KNN.\n","\n","We can also verify how the accuracy changes with various distance metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWN0Sh9V-JtR"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","params = {\n","    \"n_neighbors\": list(range(1, 10)),\n","    \"weights\": [\"uniform\", \"distance\"],\n","    \"metric\": [\"euclidean\", \"chebyshev\", \"minkowski\", \"manhattan\"]\n","}\n","\n","knn_clf = KNeighborsClassifier()\n","\n","grid_knn_clf = GridSearchCV(knn_clf, params, scoring=\"balanced_accuracy\")\n","grid_knn_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnZKWmdBp1wL"},"outputs":[],"source":["evaluate_grid(grid_knn_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"X1lTqyX-AF8X"},"source":["## Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"49CXI9yvdqTG"},"source":["Logistic regression is especially suited for binary problems and it can also be tuned for imbalanced data (by setting `class_weight=\"balanced\"`), so we expect this method to give good results.\n","\n","The `LogisticRegressionCV` class also allows to specify lists of parameters to try. Just like the `GridSearchCV` class, it evaluates each possible combination of parameters and keeps the best one, while also having noticable performance improvements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TVnrGr3CJE6"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegressionCV\n","\n","log_clf = LogisticRegressionCV(\n","    random_state=42,\n","    Cs=np.logspace(-4, 4, num=30),  # 30 items in logspace from 10^-4 to 10^4\n","    scoring=\"balanced_accuracy\",\n","    max_iter=500,\n","    class_weight=\"balanced\"\n",")\n","\n","log_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3iOmU4gp4O3"},"outputs":[],"source":["evaluate(log_clf)\n","print_confusion_matrix(y_test, log_clf.predict(X_test))\n"]},{"cell_type":"markdown","metadata":{"id":"NuwEhU4pk0me"},"source":["## SVM"]},{"cell_type":"markdown","metadata":{"id":"UD1UUdjclJiB"},"source":["*Support Vector Machines* (SVM) can be tuned for imbalanced data, and we can also check how various kernels behave."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDkUACAkrzNL"},"outputs":[],"source":["#@title SVM takes a lot of time skipped by default\n","execute_svm = False #@param {type:\"boolean\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwESlmp-8A_e"},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","if execute_svm:\n","  params = {\n","      \"C\": [1, 10, 50],\n","      \"gamma\": [\"scale\", 1, 0.1, 0.01],\n","      \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n","      \"degree\": [2, 3, 4]\n","  }\n","\n","  svm_clf = SVC(random_state=42, class_weight=\"balanced\")\n","\n","  grid_svm_clf = GridSearchCV(svm_clf, params, scoring=\"balanced_accuracy\")\n","  grid_svm_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dIyqNLFuKNN"},"outputs":[],"source":["if execute_svm:\n","  evaluate_grid(grid_svm_clf)"]},{"cell_type":"markdown","metadata":{"id":"SdWHftw3lJiB"},"source":["The resulting accuracy of the best model is high, but it's worth noticing that a considerable amount of time is required for training and cross-validation. "]},{"cell_type":"markdown","metadata":{"id":"-6u4_JAUkMUG"},"source":["## Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwTyG3TRlca2"},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","\n","params = {\n","    \"var_smoothing\": np.logspace(0, -9, num=300)\n","}\n","\n","gnb_clf = GaussianNB()\n","\n","grid_gnb_clf = GridSearchCV(gnb_clf, params, scoring=\"balanced_accuracy\")\n","grid_gnb_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDFTfSUzlJiB"},"outputs":[],"source":["evaluate_grid(grid_gnb_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"MvUEqb3issHq"},"source":["# Ensemble Classifiers"]},{"cell_type":"markdown","metadata":{"id":"zzyC0wra3pve"},"source":["## Bagging"]},{"cell_type":"markdown","metadata":{"id":"zwez9Wa50v_L"},"source":["The `BaggingClassifier` provided by scikit-learn allows to combine predictions from many base estimators.\n","However we don't expect it to be particularly good for this dataset, because it does not take in account the imbalance in the instances: the resulting classifier would have a bias towards the majority class.\n","\n","We can verify if this assumption is true:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z29-SXaw3tpq"},"outputs":[],"source":["from sklearn.ensemble import BaggingClassifier\n","\n","bag_clf = BaggingClassifier(random_state=42)\n","bag_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WiwZxXQ6rJpN"},"outputs":[],"source":["evaluate(bag_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"uroFJgXXVfB9"},"source":["The `BalancedBaggingClassifier` included in `imbalanced-learn` is more appropriate because it includes an additional step to balance the training set at fit time using a given sampler. The default sampler works by doing a random undersampling of the majority class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztn-z6RD442J"},"outputs":[],"source":["from imblearn.ensemble import BalancedBaggingClassifier\n","\n","params = {\n","    \"n_estimators\": [100, 200, 500],\n","    \"max_samples\": [0.2, 0.5, 1.0],\n","    \"max_features\": [0.5, 1.0],\n","}\n","\n","grid_bb_clf = GridSearchCV(\n","    BalancedBaggingClassifier(random_state=42),\n","    params,\n","    scoring=\"balanced_accuracy\"\n",")\n","\n","grid_bb_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pgrNB9WrMN3"},"outputs":[],"source":["evaluate_grid(grid_bb_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"PwAU-G5DG2Y0"},"source":["## Random Forest"]},{"cell_type":"markdown","metadata":{"id":"2vaPyU64XoR_"},"source":["For the same reason as basic bagging, we can expect random forests and extra trees to produce unsatisfying results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JT47jwGwHL6m"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rf_clf = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n","rf_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FFVfOhtrO6d"},"outputs":[],"source":["evaluate(rf_clf)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ebmOro7QKGAM"},"outputs":[],"source":["from sklearn.ensemble import ExtraTreesClassifier\n","\n","et_clf = ExtraTreesClassifier(random_state=42, class_weight=\"balanced\")\n","et_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8aNwCadcrRc0"},"outputs":[],"source":["evaluate(et_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"amwM3wwWsoW_"},"source":["Both method have a relatively low (balanced) accuracy, even if the class weight is set to `balanced`.\n","\n","\n","`BalancedRandomForest` should provide better results, by undersampling the majority class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zQj66Kq7x2j"},"outputs":[],"source":["from imblearn.ensemble import BalancedRandomForestClassifier\n","\n","params = {\n","    \"n_estimators\": [200, 300],\n","    \"criterion\": [\"gini\", \"entropy\"],\n","    \"max_depth\": list(range(5, 10)),\n","    \"max_features\": [\"log2\", \"sqrt\"],\n","}\n","\n","grid_brf_clf = GridSearchCV(\n","    BalancedRandomForestClassifier(random_state=42, oob_score=True),\n","    params,\n","    scoring=\"balanced_accuracy\"\n",")\n","\n","grid_brf_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mRLmPk4rUdj"},"outputs":[],"source":["evaluate_grid(grid_brf_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"UcUaBy9PFuu0"},"source":["## Random Undersampling with Boosting"]},{"cell_type":"markdown","metadata":{"id":"2IgUwb-MvEVg"},"source":["`imbalanced-learn` also includes `RUSBoostClassifier`, which does random under-sampling integrated in the learning of AdaBoost."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVwAglAmEjid"},"outputs":[],"source":["from imblearn.ensemble import RUSBoostClassifier\n","\n","params = {\n","    \"n_estimators\": [50, 100, 200],\n","    \"learning_rate\": [0.01, 0.1, 1],\n","}\n","\n","grid_rus_clf = GridSearchCV(\n","    RUSBoostClassifier(random_state=42),\n","    params,\n","    scoring=\"balanced_accuracy\"\n",")\n","\n","grid_rus_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSC-h3Lcvi9F"},"outputs":[],"source":["evaluate_grid(grid_rus_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"YZAStGDJF2Gc"},"source":["## EasyEnsemble"]},{"cell_type":"markdown","metadata":{"id":"3sWYnq-6vURV"},"source":["The final classifier included in `imbalanced-learn` is `EasyEnsemble`, that is an ensemble of AdaBoost learners trained on different balanced boostrap samples. The balancing is achieved by random under-sampling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEpZWmws8OAb"},"outputs":[],"source":["from imblearn.ensemble import EasyEnsembleClassifier\n","\n","params = {\n","    \"n_estimators\": [50, 100, 200],\n","}\n","\n","grid_ee_clf = GridSearchCV(\n","    EasyEnsembleClassifier(random_state=42),\n","    params,\n","    scoring=\"balanced_accuracy\"\n",")\n","\n","grid_ee_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2O-SAWXvlWK"},"outputs":[],"source":["evaluate_grid(grid_ee_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"ncMa9OsSIF3k"},"source":["## XGBoost"]},{"cell_type":"markdown","metadata":{"id":"Jl78pNrdlJiE"},"source":["XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting that solve many data science problems in a fast and accurate way. \n","\n","To work with imbalanced data sets, it's recommended to set the `scale_pos_weight` to the ratio of negative instances to the positive ones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmFKKM64KBdq"},"outputs":[],"source":["from collections import Counter\n","\n","counter = Counter(y)\n","estimate = counter[0] / counter[1]\n","print(\"Estimate: %.3f\" % estimate)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ciis_OPOIOrY"},"outputs":[],"source":["from xgboost import XGBClassifier\n","\n","xgb_clf = XGBClassifier(random_state=42, scale_pos_weight=estimate,\n","                        objective=\"binary:logistic\", verbosity=0)\n","\n","params = {\n","    \"n_estimators\": [50, 100],\n","    \"max_depth\": [3, 5, 10],\n","    \"learning_rate\": [0.1, 0.01],\n","    \"subsample\": [0.8, 1],\n","    \"colsample_bytree\": [0.5, 0.8]\n","}\n","\n","grid_xgb_clf = GridSearchCV(xgb_clf, params, scoring=\"balanced_accuracy\")\n","grid_xgb_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T05YFkIIqvGr"},"outputs":[],"source":["evaluate_grid(grid_xgb_clf)\n"]},{"cell_type":"markdown","metadata":{"id":"mi4hQWJjXO6S"},"source":["## Voting"]},{"cell_type":"markdown","metadata":{"id":"clIG1aIplJiF"},"source":["Finally, we can combine all the previous ensemble classifiers into a *Voting Ensemble*.\n","We will begin with a hard voting ensemble, where each classifier has the same weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLxfaUem_TFz"},"outputs":[],"source":["from sklearn.ensemble import VotingClassifier\n","\n","\n","def get_models():\n","    models = list()\n","    models.append((\"bb\",  grid_bb_clf.best_estimator_))\n","    models.append((\"brf\",  grid_brf_clf.best_estimator_))\n","    models.append((\"rus\",  grid_rus_clf.best_estimator_))\n","    models.append((\"ee\",  grid_ee_clf.best_estimator_))\n","    models.append((\"xgb\",  grid_xgb_clf.best_estimator_))\n","    return models\n","\n","\n","hard_voting_clf = VotingClassifier(estimators=get_models(), voting=\"hard\")\n","hard_voting_clf.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJphzWm9lJiF"},"outputs":[],"source":["evaluate(hard_voting_clf)\n","print_confusion_matrix(y_test, hard_voting_clf.predict(X_test))\n"]},{"cell_type":"markdown","metadata":{"id":"TSx0DmjclJiF"},"source":["We can also create a soft voting classifier, where the weights of the classifiers are determined by the accuracy of the individual classifiers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RfVQa8HlJiF"},"outputs":[],"source":["# relative weights\n","model_scores = {\n","   name: balanced_accuracy_score(\n","      y_train,\n","      model.predict(X_train),\n","   )\n","   for name, model in get_models()\n","}\n","total_score = sum(model_scores.values())\n","\n","soft_voting_clf = VotingClassifier(estimators=get_models(), voting=\"soft\",  weights=[\n","    model_scores[name] / total_score\n","    for name, _ in get_models()\n","  ])\n","soft_voting_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lNzI1I1lJiF"},"outputs":[],"source":["evaluate(soft_voting_clf)\n","print_confusion_matrix(y_test, soft_voting_clf.predict(X_test))"]},{"cell_type":"markdown","metadata":{"id":"CoOSH3_alJiF"},"source":["The hard voting ensemble has better results.\n","\n","To estimate the effectiveness of the voting ensembe we can cross validate the single models, alongside the voting ensembel.\n","Then we use a boxplot to show the accuracy of each cross-validated model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICnQBT7vlJiF"},"outputs":[],"source":["from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import cross_val_score\n","\n","\n","def evaluate_model(model):\n","    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n","    scores = cross_val_score(\n","        model, X, y, scoring=\"balanced_accuracy\", cv=cv, n_jobs=-1, error_score=\"raise\")\n","    return scores\n","\n","\n","eval_list = get_models()\n","eval_list.append((\"hard\", hard_voting_clf))\n","eval_list.append((\"soft\", soft_voting_clf))\n","\n","results, names = list(), list()\n","for name, model in eval_list:\n","    scores = evaluate_model(model)\n","    results.append(scores)\n","    names.append(name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5l96MqXFI0Ld"},"outputs":[],"source":["plt.boxplot(results, labels=names, showmeans=True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"D61mcmrHlJiG"},"source":["The voting ensembles have a slightly lower accuracy on the test set than the xgb classifier, but they do have a considerably lower variance."]},{"cell_type":"markdown","metadata":{"id":"fM0zo4ZP8p6T"},"source":["# Neural Network"]},{"cell_type":"markdown","metadata":{"id":"hVvuSKTM29Hj"},"source":["## Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNy1pWkY8crZ"},"outputs":[],"source":["def plot_results(history):\n","    \"\"\"\n","    Plots the results of the model training\n","\n","    :param history: object returned by the train function that contains all train infos\n","    \"\"\"\n","    history_dict = history.history\n","\n","    loss_values = history_dict['loss']\n","    val_loss_values = history_dict['val_loss']\n","\n","    epochs = range(1, len(loss_values) + 1)\n","\n","    # Plot line charts for both Validation and Training Loss\n","    line1 = plt.plot(epochs, val_loss_values, label='Validation/Test Loss')\n","    line2 = plt.plot(epochs, loss_values, label='Training Loss')\n","    plt.setp(line1, linewidth=2.0, marker='+', markersize=10.0)\n","    plt.setp(line2, linewidth=2.0, marker='4', markersize=10.0)\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.grid(True)\n","    plt.legend()\n","    plt.show()\n","\n","    history_dict = history.history\n","\n","    acc_values = history_dict['balanced_binary_accuracy']\n","    val_acc_values = history_dict['val_balanced_binary_accuracy']\n","\n","    epochs = range(1, len(loss_values) + 1)\n","\n","    line1 = plt.plot(epochs, val_acc_values, label='Validation/Test Accuracy')\n","    line2 = plt.plot(epochs, acc_values, label='Training Accuracy')\n","    plt.setp(line1, linewidth=2.0, marker='+', markersize=10.0)\n","    plt.setp(line2, linewidth=2.0, marker='4', markersize=10.0)\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.grid(True)\n","    plt.legend()\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ES4GexY17anw"},"source":["To compile the mopdel as suggested in https://carpentries-incubator.github.io/deep-learning-intro/02-keras/index.html, it is possibile to use a precompiled model rather than compiling a new one.  \n","Since the model is simple and requires very little computation time, we will build it from scratch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoqHqp6S7M79"},"outputs":[],"source":["class BalancedBinaryAccuracy(keras.metrics.BinaryAccuracy):\n","    def __init__(self, name='balanced_binary_accuracy', dtype=None):\n","        super().__init__(name, dtype=dtype)\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_flat = y_true\n","        if y_true.shape.ndims == y_pred.shape.ndims:\n","            y_flat = tf.squeeze(y_flat, axis=[-1])\n","        y_true_int = tf.cast(y_flat, tf.int32)\n","\n","        cls_counts = tf.math.bincount(y_true_int)\n","        cls_counts = tf.math.reciprocal_no_nan(tf.cast(cls_counts, self.dtype))\n","        weight = tf.gather(cls_counts, y_true_int)\n","        return super().update_state(y_true, y_pred, sample_weight=weight)"]},{"cell_type":"markdown","metadata":{"id":"X_KcQoWxlJiG"},"source":["To validate the performance of the neural network we use `train_test_split` to generate a validation set, which is composed by the `20%` of the training set. The original testing set is used as is."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2zzBn2olJiG"},"outputs":[],"source":["X_test_nn, y_test_nn = X_test, y_test\n","\n","X_train_nn, X_val_nn, y_train_nn,  y_val_nn = train_test_split(\n","    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"ZLRCBCfZ1ahC"},"source":["## Base NN"]},{"cell_type":"markdown","metadata":{"id":"HxXHswvpwQIe"},"source":["To easily create a basic Neural Network, `keras` can be used.\n","To test the effectiveness of a Neural Network to this particular problem, we create a baseline neural network with one hidden layer of 10 nodes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eu1Ddcg18s7i"},"outputs":[],"source":["from tensorflow import keras\n","\n","model = keras.models.Sequential([\n","    # The number of inputs has to be equal\n","    # to the number of features of the dataset\n","    keras.Input(shape=X_train.shape[1]),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # One output node, which is a sigmoid:\n","    # the value will be continuous between 0 and 1.\n","    # To cast it to binary, we will need to assign\n","    # value 1 if greater than 0.5, or else 0.\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"PM6z1tM2_VaJ"},"source":["Now that the neural network has been defined, it needs to be compiled.\n","\n","`Adam` is suggested as a sensible optimizer for the network. \n","\n","Since the output needs to be binary, `BinaryCrossentropy()` is an appropriate loss function.\n","\n","We also want to use the balanced accuracy to measure the performance of the neural network.\n","Since it's not defined in the Keras library, we will need to define it ourselves.   \n","\n","We also save the weights so we will be able to load the starting weights without the need to re-compile the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cw0ausM8-_xL"},"outputs":[],"source":["model.compile(\n","    optimizer=\"adam\",\n","    loss=keras.losses.BinaryCrossentropy(),\n","    metrics=[BalancedBinaryAccuracy()]\n",")\n","\n","model.save_weights('model.h5')"]},{"cell_type":"markdown","metadata":{"id":"qlKtTbG0JMLh"},"source":["The compiled model can be now trained: as a test, we will run it with 10 epochs. This number is, once again, arbitrary and will most likely need to be tweaked.\n","\n","The `y_train` list cannot however be used directly, because the neural network\n","cannot directly output a categorical value: to overcome this, the `y_train`\n","list is transformed using Hot-Encoding."]},{"cell_type":"markdown","metadata":{"id":"lZ4avHiG3HAx"},"source":["### 10 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rThMQ6Pk5W8q"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7V2YmvPB_GIQ"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=10,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"D5w1e0NhKOvA"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"RrnOdjVv8ZgH"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXANsBcZ8kO2"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"fhVo8qDzMeJL"},"source":["The graph shows that the loss function, at 10 epochs, is still decreasing:\n","a higher number of epochs, with the same model configuration, should lower\n","the loss function value, and improve the quality of the predictions.\n","\n","A too high number of epochs however may lead to overfitting.\n","\n","We need to check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuOdzU3tAo9-"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"466mVN5lND3w"},"source":["We can see that the accuracy is quite low.\n","Parameter tuning may allow to find a neural network with higher accuracy.\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ly-Bk3aI-L8C"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"sp4xQAyq3Pq2"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"vM-vi_2y3Qah"},"source":["### 20 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yRotM175fM2"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KboqXg-B3Qah"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=20,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"Nt1gVN5C3Qah"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"nynJ9-tr3Qah"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1U7MovV3Qah"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"v5NJYk283Qah"},"source":["As we can see at 20 epochs the loss function is decreasing but it's slowing so we might be close to the optimal number of epochs\n","\n","Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8C0hsJv13Qah"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"8RuXEHdB3Qai"},"source":["We had an increase of almost 10% with 10 additional epochs. Still not an optimal result but it's improving\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ri1p-tzx3Qai"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"YqvWeqk-4V_6"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"TVmC4y4J4Wh5"},"source":["### 40 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXyeP2H96Oq6"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWVzNG4x4Wh6"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=40,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"RyYKSsk24Wh6"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"yS16ZaSE4Wh6"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQyt-1WG4Wh6"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"-93CcUKU4Wh6"},"source":["Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-LVHgGg4Wh6"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"KHWUesG24Wh6"},"source":["Still an smaller increase in accuracy. But 40 epochs for the fit takes almost a minute to complete. We might try to improve the structure of the NN\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZnSUhBW4Wh6"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"acp67p8k711p"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"oTHLjSxm72P8"},"source":["## NN 2.0"]},{"cell_type":"markdown","metadata":{"id":"TkYx_sNC89dV"},"source":["We are simply adding a second Dense layer to reduce the number of epochs needed.  \n","We might risk overfitting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZCvZ08l72P9"},"outputs":[],"source":["from tensorflow import keras\n","\n","model = keras.models.Sequential([\n","    # The number of inputs has to be equal\n","    # to the number of features of the dataset\n","    keras.Input(shape=X_train.shape[1]),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # One output node, which is a sigmoid:\n","    # the value will be continuous between 0 and 1.\n","    # To cast it to binary, we will need to assign\n","    # value 1 if greater than 0.5, or else 0.\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"Wp4sSHGN72P9"},"source":["We compile and save the model as the Base NN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rT1AIyvG72P9"},"outputs":[],"source":["model.compile(\n","    optimizer=\"adam\",\n","    loss=keras.losses.BinaryCrossentropy(),\n","    metrics=[BalancedBinaryAccuracy()]\n",")\n","\n","model.save_weights('model.h5')"]},{"cell_type":"markdown","metadata":{"id":"qt07MhVS72P9"},"source":["The compiled model can be now trained: as a test, we will run it with 10 epochs. This number is, once again, arbitrary and will most likely need to be tweaked.\n","\n","The `y_train` list cannot however be used directly, because the neural network\n","cannot directly output a categorical value: to overcome this, the `y_train`\n","list is transformed using Hot-Encoding."]},{"cell_type":"markdown","metadata":{"id":"7giDg9XX72P9"},"source":["### 10 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ok1CWyvi72P9"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5YruYnm72P9"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=10,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"C6ABBRaA72P9"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss and accuracy function."]},{"cell_type":"markdown","metadata":{"id":"KQ0RZYIs72P-"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnhiNT3S72P-"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"YS4bXrqy72P-"},"source":["The loss function, at 10 epochs, is still decreasing:\n","a higher number of epochs, with the same model configuration, should lower\n","the loss function value, and improve the quality of the predictions.\n","\n","We can also see that the function is decreasing faster than the Base NN\n","\n","We need to check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xCypsBU72P-"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"hPsHwyGz72P-"},"source":["We can see that with 10 epochs we are already over the 70% of accuracy. More epochs might bring it to 80%\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAY0JeR-72P-"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"dMfRdLTc72P-"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"KTarMlYT72P-"},"source":["### 20 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dg2Vefws72P-"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1K7V79j72P-"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=20,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"aKhC4DsA72P-"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"IJWIrdJo72P_"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IeDPFB1x72P_"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"UCoD1Vr772P_"},"source":["Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFVgDJbH72P_"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"QHVtdoCT72P_"},"source":["We had an increase of almost 10% with 10 additional epochs. Still not an optimal result but it's improving\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fz1-zNA672P_"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"XrvIFqWU72P_"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Aiw2u93X72P_"},"source":["### 40 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cw5p7Aol72P_"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Eab2QG_72P_"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=40,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"TIMfP0Qt72P_"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"bfCkpak272P_"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eLwNxJr72QA"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"5lZqAFjq72QA"},"source":["The loss function is now decresing very slowly\n","\n","Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsabCix_72QA"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"N1jPqCgD72QA"},"source":["We are almost at 80% but since the loss function is decreasing very slowly let's try to improve the structure of the NN\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHEWQ7-372QA"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"RgFBSr5g-_Hd"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"LiL49SiOAwXb"},"source":["### Summary\n","Compared to the Base NN we have seen an improvement in accuracy of 5-10% so it's a better structure. Still we could do better"]},{"cell_type":"markdown","metadata":{"id":"ABmyaPGy-_au"},"source":["## NN 2.1"]},{"cell_type":"markdown","metadata":{"id":"myDDdVHF-_au"},"source":["Let's keep the two layers but increase the number of units of the first hidden Dense layer.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sy9lVtBq-_au"},"outputs":[],"source":["from tensorflow import keras\n","\n","model = keras.models.Sequential([\n","    # The number of inputs has to be equal\n","    # to the number of features of the dataset\n","    keras.Input(shape=X_train.shape[1]),\n","\n","    # Hidden layer\n","    keras.layers.Dense(20, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(20, activation=\"relu\"),\n","    \n","    keras.layers.Dropout(0.1),\n","\n","    # One output node, which is a sigmoid:\n","    # the value will be continuous between 0 and 1.\n","    # To cast it to binary, we will need to assign\n","    # value 1 if greater than 0.5, or else 0.\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"tTwTKWLS-_au"},"source":["We see that we now have almost double the number of trainable parameters.  \n","\n","We compile and save the model as the Base NN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBInzIBF-_av"},"outputs":[],"source":["model.compile(\n","    optimizer=\"adam\",\n","    loss=keras.losses.BinaryCrossentropy(),\n","    metrics=[BalancedBinaryAccuracy()]\n",")\n","\n","model.save_weights('model.h5')"]},{"cell_type":"markdown","metadata":{"id":"Gs_40We1-_av"},"source":["The compiled model can be now trained: as a test, we will run it with 10 epochs. This number is, once again, arbitrary and will most likely need to be tweaked.\n","\n","The `y_train` list cannot however be used directly, because the neural network\n","cannot directly output a categorical value: to overcome this, the `y_train`\n","list is transformed using Hot-Encoding."]},{"cell_type":"markdown","metadata":{"id":"qLYFVj1h-_av"},"source":["### 10 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNGFFhvi-_av"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRm0NLuS-_av"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=10,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"JlbOyVVb-_av"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss and accuracy function."]},{"cell_type":"markdown","metadata":{"id":"ZU88bzAU-_av"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXe6tNLH-_av"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"VQBL_HPZ-_av"},"source":["The loss function, at 10 epochs, is still decreasing:\n","a higher number of epochs, with the same model configuration, should lower\n","the loss function value, and improve the quality of the predictions.\n","\n","We see that the function is decreasing even faster than the Base NN or 2.0\n","\n","We need to check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wygyKbho-_av"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"5Rjf7p7K-_av"},"source":["Accuracy didn't improve much compared to the 2.0 but we might see bigger improvements with more epochs\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrRiWMf2-_av"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"22IX9pTr-_aw"},"source":["### 20 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTiOx2J_-_aw"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_hVje0I-_aw"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=20,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"e6hkpsLj-_aw"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"0Ln3Ev4J-_aw"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDXRjNDB-_aw"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"c0D82_q8-_aw"},"source":["The loss function is still decreasing so we might still see better results at 40 epochs\n","\n","Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyNN913p-_aw"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"5nANvNiy-_aw"},"source":["We had an increase of almost 10% with 10 additional epochs. Still not an optimal result but it's improving\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wNvTvQQ-_aw"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"k0WKs4tU-_aw"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"sgTtS16x-_aw"},"source":["### 40 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYnE1JuN-_aw"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Om953olD-_aw"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=40,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"ckb03q4R-_ax"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"0AZlv096-_ax"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amXRXoKD-_ax"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"PIklNtvM-_ax"},"source":["The loss function is now decresing very slowly\n","\n","Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw_mFGeh-_ax"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"weUDr6LV-_ax"},"source":["As in 2.0 we are still under 80% so we prefer the simpler NN\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZoYJybU-_ax"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"kmIaJLeLCERX"},"source":["### Summary\n","We didn't see particular improvements from the 2.0"]},{"cell_type":"markdown","metadata":{"id":"fESguAn-CQcB"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"piRODec9CQ_R"},"source":["## NN 3.0"]},{"cell_type":"markdown","metadata":{"id":"rqtAnj77CQ_S"},"source":["Let's try with 4 Dense layers.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALju9wkZCQ_S"},"outputs":[],"source":["from tensorflow import keras\n","\n","model = keras.models.Sequential([\n","    # The number of inputs has to be equal\n","    # to the number of features of the dataset\n","    keras.Input(shape=X_train.shape[1]),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # One output node, which is a sigmoid:\n","    # the value will be continuous between 0 and 1.\n","    # To cast it to binary, we will need to assign\n","    # value 1 if greater than 0.5, or else 0.\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"2cft-8CcCQ_S"},"source":["We compile and save the model as the Base NN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-F3kZDoyCQ_S"},"outputs":[],"source":["model.compile(\n","    optimizer=\"adam\",\n","    loss=keras.losses.BinaryCrossentropy(),\n","    metrics=[BalancedBinaryAccuracy()]\n",")\n","\n","model.save_weights('model.h5')"]},{"cell_type":"markdown","metadata":{"id":"ErJeM_AQCQ_S"},"source":["The compiled model can be now trained: as a test, we will run it with 10 epochs. This number is, once again, arbitrary and will most likely need to be tweaked.\n","\n","The `y_train` list cannot however be used directly, because the neural network\n","cannot directly output a categorical value: to overcome this, the `y_train`\n","list is transformed using Hot-Encoding."]},{"cell_type":"markdown","metadata":{"id":"Ma-HkqR2CQ_S"},"source":["### 10 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tswBcRf-CQ_S"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQTcn1EZCQ_S"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=10,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"oik4lTnuCQ_S"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss and accuracy function."]},{"cell_type":"markdown","metadata":{"id":"5B68oYdjCQ_S"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9AABUFcCQ_S"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"Uxfi7w9tCQ_T"},"source":["The loss function, at 10 epochs, is still decreasing:\n","a higher number of epochs, with the same model configuration, should lower\n","the loss function value, and improve the quality of the predictions.\n","\n","We see that the function is decreasing even faster than the Base NN or 2.0\n","\n","We need to check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVWfvUecCQ_T"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"nFTIeYF4CQ_T"},"source":["Accuracy reached 76% in just 10 epochs. 5% better than 2.0\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yCnvuvuCQ_T"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"-ereZLnNCQ_T"},"source":["### 20 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"muJ1zf_zCQ_T"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FH4qh1iHCQ_T"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=20,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"OA0xDTqVCQ_T"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"b5g5L-DyCQ_T"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTot4PCcCQ_T"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"vkpaC03CCQ_T"},"source":["The loss function is still decreasing so we might still see better results at 40 epochs\n","\n","Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gDVAy9ACQ_T"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"i91x86unCQ_T"},"source":["We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6AQfXRdCQ_T"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"_wkaWM_cCQ_U"},"source":["### 40 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOVdqr6-CQ_U"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AdYNHxvPCQ_U"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=40,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"Ao6oQy-FCQ_U"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"um0K7RXUCQ_U"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuVnCggfCQ_U"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"LLGBgP6WCQ_U"},"source":["The loss function is now decresing very slowly\n","\n","Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ko0Vm39ACQ_U"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"adIa7IDCCQ_U"},"source":["As in 2.0 we are still under 80% so we prefer the simpler NN\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3pu3tQRCQ_U"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"S7RR98ROCQ_V"},"source":["### Summary\n","We didn't see particular accuracy improvents overall but we reach 76% in just 10 epochs."]},{"cell_type":"markdown","metadata":{"id":"qOT8LN1EGYkm"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"a2fbhOSgGY6l"},"source":["## NN 3.1"]},{"cell_type":"markdown","metadata":{"id":"EXI6cu48GY6m"},"source":["Let's try with 4 Dense layers and Dropout Layers.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1fM_aYdGY6m"},"outputs":[],"source":["from tensorflow import keras\n","\n","model = keras.models.Sequential([\n","    # The number of inputs has to be equal\n","    # to the number of features of the dataset\n","    keras.Input(shape=X_train.shape[1]),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # Hidden layer\n","    keras.layers.Dense(10, activation=\"relu\"),\n","\n","    # One output node, which is a sigmoid:\n","    # the value will be continuous between 0 and 1.\n","    # To cast it to binary, we will need to assign\n","    # value 1 if greater than 0.5, or else 0.\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"1CZqV5W6GY6m"},"source":["We compile and save the model as the Base NN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aK25B2cwGY6m"},"outputs":[],"source":["model.compile(\n","    optimizer=\"adam\",\n","    loss=keras.losses.BinaryCrossentropy(),\n","    metrics=[BalancedBinaryAccuracy()]\n",")\n","\n","model.save_weights('model.h5')"]},{"cell_type":"markdown","metadata":{"id":"qKQ6PZwFGY6m"},"source":["The compiled model can be now trained: as a test, we will run it with 10 epochs. This number is, once again, arbitrary and will most likely need to be tweaked.\n","\n","The `y_train` list cannot however be used directly, because the neural network\n","cannot directly output a categorical value: to overcome this, the `y_train`\n","list is transformed using Hot-Encoding."]},{"cell_type":"markdown","metadata":{"id":"fSZN5lFnGY6m"},"source":["### 10 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOuGNJi9GY6m"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8oO0SQHGY6m"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=10,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"IJfBb4ctGY6m"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss and accuracy function."]},{"cell_type":"markdown","metadata":{"id":"l7dTvm-OGY6m"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2WC8DVjGY6n"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"HJWPZq3zGY6n"},"source":["The loss function, at 10 epochs, is still decreasing:\n","a higher number of epochs, with the same model configuration, should lower\n","the loss function value, and improve the quality of the predictions.\n","\n","We see that the function is decreasing even faster than the Base NN or 2.0\n","\n","We need to check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfKFX6pFGY6n"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"Qc9lL4g3GY6n"},"source":["Accuracy reached 76% in just 10 epochs. 5% better than 2.0\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBJp5f-iGY6n"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"qLWCY2EkGY6n"},"source":["### 20 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daV3AfOpGY6n"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6SGBpJoGY6n"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=20,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"SI3UukqQGY6n"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"zJ_cuT1NGY6n"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UVRFWxAGY6n"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"lZwoAwRoGY6n"},"source":["The loss function is still decreasing so we might still see better results at 40 epochs\n","\n","Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsUBFvFqGY6o"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"hRQlfANsGY6o"},"source":["We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qV-4nXpNGY6o"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"J260aDy4GY6o"},"source":["### 40 Epochs Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnU8PmPSGY6o"},"outputs":[],"source":["model.load_weights('model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfYv7tjyGY6o"},"outputs":[],"source":["history = model.fit(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=40,\n","    validation_data=(X_val_nn, y_val_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"mP9jWoveGY6o"},"source":["To evaluate the model we can begin by looking at evolution of the value of the loss\n","function."]},{"cell_type":"markdown","metadata":{"id":"x2q51cGjGY6o"},"source":["#### Plot Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwKa7yKYGY6o"},"outputs":[],"source":["plot_results(history)\n"]},{"cell_type":"markdown","metadata":{"id":"3Ek991utGY6o"},"source":["The loss function is now decresing very slowly\n","\n","Let's check the accuracy on the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGPWpHZ0GY6o"},"outputs":[],"source":["y_pred_nn = model.predict(X_test_nn).flatten()\n","y_pred_nn = pd.Series(y_pred_nn).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test_nn, y_pred_nn))\n"]},{"cell_type":"markdown","metadata":{"id":"ZNtLroaAGY6o"},"source":["As in 2.0 we are still under 80% so we prefer the simpler NN\n","\n","We can also plot a heatmap of the confusion matrix of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQzcd4B-GY6o"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","y_pred = model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"ju7Z2DxgGY6p"},"source":["### Summary\n","We didn't see particular accuracy improvents overall but we reach 76% in just 10 epochs."]},{"cell_type":"markdown","metadata":{"id":"jpyjuetkG2ge"},"source":["## Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"anqGBd4sG8fl"},"source":["To tune the hyperparameters of a keras neural network, we can use the `keras-tuner` package."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61obp8wDsWap"},"outputs":[],"source":["from keras import layers\n","!pip install keras-tuner\n","import keras_tuner\n","\n","\n","def build_model(hp):\n","    # Initialize the Sequential API and start stacking the layers\n","    model = keras.Sequential()\n","\n","    # Input layer\n","    model.add(layers.Flatten())\n","\n","    # Select number of hidden layer, between 1 and 3\n","    for i in range(hp.Int(\"num_layers\", 1, 3)):\n","        model.add(\n","            layers.Dense(\n","                # Tune number of units separately.\n","                # Choose an optimal value between 32-512\n","                units=hp.Int(f\"units_{i}\", min_value=32,\n","                             max_value=512, step=32),\n","                activation=\"relu\"\n","            )\n","        )\n","        # Tune whether to use dropout at the end of this layer\n","        if hp.Boolean(f\"dropout_{i}\"):\n","            model.add(layers.Dropout(rate=0.25))\n","\n","    # Output layer\n","    model.add(layers.Dense(1, activation=\"sigmoid\"))\n","\n","    # Define the optimizer learning rate as a hyperparameter.\n","    learning_rate = hp.Float(\"lr\", min_value=1e-4,\n","                             max_value=1e-2, sampling=\"log\")\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss=keras.losses.BinaryCrossentropy(),\n","                  metrics=[BalancedBinaryAccuracy()])\n","\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"HutVkY11lJiI"},"source":["Now we can run the search for the best parameters.\n","We use the `hyperband` search algorithm, which randomly samples all the combinations of hyperparameters and instead of running full training and evaluation on the models, it trains each model for a few epochs with these combinations and select the best candidates based on the results on these few epochs. It does this iteratively and finally runs full training and evaluation on the final chosen candidates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvpmbPRy4bU-"},"outputs":[],"source":["tuner = keras_tuner.Hyperband(\n","    hypermodel=build_model,\n","    objective=keras_tuner.Objective(\n","        \"val_balanced_binary_accuracy\", direction=\"max\"),\n","    max_epochs=30,\n","    directory=\"mldm_nn\",\n","    project_name=\"mldm_nn_hyperband\",\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRgUXdb-4iPZ"},"outputs":[],"source":["from IPython.display import clear_output\n","\n","\n","class ClearTrainingOutput(keras.callbacks.Callback):\n","    def on_train_end(*args, **kwargs):\n","        clear_output(wait=True)\n","\n","\n","tuner.search(\n","    X_train_nn,\n","    y_train_nn,\n","    epochs=100,\n","    shuffle=True,\n","    verbose=1,\n","    use_multiprocessing=True,\n","    workers=2,\n","    callbacks=[ClearTrainingOutput()],\n","    validation_data=(X_val_nn, y_val_nn)\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1Dp3fS28dUj"},"outputs":[],"source":["tuner.results_summary()\n"]},{"cell_type":"markdown","metadata":{"id":"NwJzOB_flJiI"},"source":["We can now see the optimal neural network configuration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXBbdece5YAh"},"outputs":[],"source":["# Get the top model.\n","best_model = tuner.get_best_models(num_models=1)[0]\n","\n","# Build the model.\n","# Needed for `Sequential` without specified `input_shape`.\n","best_model.build(input_shape=X.shape)\n","best_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-i5elClb85MG"},"outputs":[],"source":["y_pred = best_model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print(\"Accuracy: \", balanced_accuracy_score(y_test, y_pred))\n"]},{"cell_type":"markdown","metadata":{"id":"WeZZusNblJiI"},"source":["The accuracy is not as high as the one obtained with the ensemble classifiers, but it's still quite high.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wk07xRUN9Cah"},"outputs":[],"source":["y_pred = best_model.predict(X_test).flatten()\n","y_pred = pd.Series(y_pred).map(lambda y: 1 if y >= 0.5 else 0)\n","\n","print_confusion_matrix(y_test, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"XjOJ21DxR5FH"},"source":["#### Summary\n","80% accuracy better but with much simpler models we still obtained almost 78%"]},{"cell_type":"markdown","metadata":{"id":"DnR8XLpcc4ak"},"source":["# Autosklearn"]},{"cell_type":"markdown","metadata":{"id":"yCee-1r2lJiJ"},"source":["Auto-sklearn is an automated machine learning toolkit, which tries to identify the best possible model for a given dataset.\n","It is usually used as a starting point, from which the best model is improved manually.\n","In this project however we will use Auto-sklearn to compare its results to ours.\n","\n","We use Auto-sklearn 2.0, which at the time of writing is still in the experimental stage. It should anyway perform better than the previous version. \n","We set `12` hours as the time limit for the execution, with a maximum of `30` minutes per run.\n","\n","Once again, the models will be evaluated using balanced accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qovjkzR4dKDy"},"outputs":[],"source":["from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n","from autosklearn.metrics import balanced_accuracy\n","\n","auto_cls = AutoSklearn2Classifier(\n","    time_left_for_this_task=60*60*12,  # 12 hours\n","    per_run_time_limit=60*30,  # 30 minutes\n","    memory_limit=1024*3,  # 3 GB\n","    metric=balanced_accuracy,\n","    seed=42,\n",")\n","auto_cls.fit(X_train, y_train)\n"]},{"cell_type":"markdown","metadata":{"id":"r-UglArDlJiJ"},"source":["We can see the statistic of the Auto-sklearn execution using the `autosklearn` method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPledv9hlJiJ"},"outputs":[],"source":["print(auto_cls.sprint_statistics())\n"]},{"cell_type":"markdown","metadata":{"id":"uwVBb7yLlJiJ"},"source":["Using the `leaderboard` method we can see the ranking of the best models found.\n","\n","To get the best performance out of the evaluated models, wuto-sklearn builds an ensemble based on the models’ prediction for the validation set.\n","The `ensemble_weight` column represents the weight of the single model on the ensemble, `cost` the value of the loss function associated with the model, `duration` the length of time the model was optimized for."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ifzkKPzlJiJ"},"outputs":[],"source":["auto_cls.leaderboard()\n"]},{"cell_type":"markdown","metadata":{"id":"s5-BBGcslJiJ"},"source":["The best model(s) found by `autosklearn` are the ones using gradient boosting: this is consistent with our results, where we identified XGBoost (which is a variation of gradient boosting) as the most accurate model.\n","\n","We can also plot the accuracy over time:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgR8vA7JlJiJ"},"outputs":[],"source":["poT = auto_cls.performance_over_time_\n","poT.plot(\n","    x='Timestamp',\n","    kind='line',\n","    legend=True,\n","    title='Auto-sklearn accuracy over time',\n","    grid=True,\n",")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"iVQiiFsklJiK"},"source":["Finally, we evaluate the accuracy of the best auto-sklearn model on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NVWQU_VlJiK"},"outputs":[],"source":["evaluate(auto_cls)\n"]},{"cell_type":"markdown","metadata":{"id":"pBDudVyRlJiK"},"source":["As we can see, the accuracy is close to the one of the XGBoost classifier."]}],"metadata":{"colab":{"collapsed_sections":["fQAuFIojY4Ac","cwIqoaD8QtNl","4dvXiMXBV4qg","p3oVkntVZrWF","FLBjsIU-QtNo","H-Fy1Gg-QtNv","k33XbElQ7mWb","rV09yQBPaI4q","feXvsTHcbIHn","UspP5qDpM4Hu","lf9_S-b17a6C","aaToOTjfHN5T","0JoKHo7mQjuu","vmNnHhHVJG5A","FeHkqxul9dMx","Nxn0hNB3QtNy","pumyaOvphvc7","OPGq2ec-8YTT","-6u4_JAUkMUG","jpyjuetkG2ge","T7CEKABQYolM","Pv6kW9ivYq35","DnR8XLpcc4ak"],"name":"MLDM_Beatrice_Cotti.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 ('MLDM_Project')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"nav_menu":{"height":"279px","width":"309px"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"toc_cell":false,"toc_position":{},"toc_section_display":"block","toc_window_display":false},"vscode":{"interpreter":{"hash":"46d17d6a279f262b4e3ac0ae1e56cd888970469ea13e69cd0d25c7c72c9ebbac"}}},"nbformat":4,"nbformat_minor":0}